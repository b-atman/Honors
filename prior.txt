#Using a Discrete Prior
library('LearnBayes')
p <- seq(0.05, 0.95, by = 0.1)
prior <- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
prior <- prior / sum(prior)
plot(p, prior, type = "h", ylab="Prior Probability")


#The posterior for p –
data <- c(11, 16)
post <- pdisc(p, prior, data)
round(cbind(p, prior, post),2)


library(lattice)
PRIOR <- data.frame("prior", p, prior)
POST <- data.frame("posterior", p, post)
names(PRIOR) <- c("Type", "P", "Probability")
names(POST) <- c("Type","P","Probability")
data <- rbind(PRIOR, POST)
xyplot(Probability ~ P | Type, data=data, 
 layout=c(1,2), type="h", lwd=3, col="black")


#Using a Beta Prior
#Construct a beta prior for p by inputting two percentiles –
quantile2 <- list(p=.9, x=.5)
quantile1 <- list(p=.5, x=.3)
(ab <- beta.select(quantile1,quantile2))


#Bayesian triplot –
a <- ab[1]
b <- ab[2]
s <- 11
f <- 16
curve(dbeta(x, a + s, b + f), from=0, to=1, xlab="p", ylab="Density", lty=1, lwd=4)
curve(dbeta(x, s + 1, f + 1), add=TRUE, lty=2, lwd=4)
curve(dbeta(x, a, b), add=TRUE, lty=3, lwd=4)
legend(.7, 4, c("Prior", "Likelihood", "Posterior"), lty=c(3, 2, 1), lwd=c(3, 3, 3))


#Posterior summaries –
1 - pbeta(0.5, a + s, b + f)
qbeta(c(0.05, 0.95), a + s, b + f)


#Simulating from posterior –
ps <- rbeta(1000, a + s, b + f)
hist(ps, xlab="p")

sum(ps >= 0.5) / 1000

quantile(ps, c(0.05, 0.95))


#Using a Histogram Prior
#Beliefs about p are expressed by a histogram prior. 
midpt <- seq(0.05, 0.95, by = 0.1)
prior <- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 
 0.1, 0, 0)
prior <- prior / sum(prior)
curve(histprior(x, midpt, prior), from=0, to=1, ylab="Prior density", ylim=c(0, .3))
curve(histprior(x,midpt,prior) * dbeta(x, s + 1, f + 1), from=0, to=1, ylab="Posterior density")


p <- seq(0, 1, length=500)
post <- histprior(p, midpt, prior) * dbeta(p, s + 1, f + 1)
post <- post / sum(post)
ps <- sample(p, replace = TRUE, prob = post)
hist(ps, xlab="p", main="")


#Prediction
#Want to predict the number of heavy sleepers in a future sample of 20.
#Discrete prior approach –
p <- seq(0.05, 0.95, by=.1)
prior <- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0)
prior <- prior / sum(prior)
m <- 20
ys <- 0:20
pred <- pdiscp(p, prior, m, ys)
cbind(0:20, pred)

#Continuous prior approach –
ab <- c(3.26, 7.19)
m <- 20
ys <- 0:20
pred <- pbetap(ab, m, ys)
#Simulating predictive distribution –
p <- rbeta(1000, 3.26, 7.19)
y <- rbinom(1000, 20, p)
table(y)

freq <- table(y)
ys <- as.integer(names(freq))
predprob <- freq / sum(freq)
plot(ys, predprob, type="h", xlab="y", ylab="Predictive Probability")


#Construction of a prediction interval –
dist <- cbind(ys, predprob)
covprob <- .9
discint(dist, covprob)

